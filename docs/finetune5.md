# ğŸš€ Fineâ€‘Tuning Journey: From Raw Text to ğŸ¯ Accurate Model

Ever wondered **what actually happens** when we say *"we fineâ€‘tuned the model"*?  Below is a friendly, emojiâ€‘powered walkâ€‘through of the entire pipeline so you can share it at standâ€‘ups or in a PR description.

---
## ğŸ—ºï¸ Highâ€‘Level Map

| ğŸ Step | ğŸ“‚ File | ğŸ› ï¸ What It Does |
|---------|---------|-----------------|
| 1ï¸âƒ£  | `cleaner_finetune.py` | ğŸ§¹ Cleans & balances raw JSONL, then ğŸšš uploads it as a HF dataset (`train`/`test`). |
| 2ï¸âƒ£  | `libdocs/finetune/finetune.py` | ğŸ‹ï¸â€â™‚ï¸ Fineâ€‘tunes a base model on the `train` split, pushes the ğŸ¯ new model to HF Hub. |
| 3ï¸âƒ£  | `libdocs/finetune/run.py` | ğŸ”¬ Evaluates the pushed model on the heldâ€‘out `test` split, logs topâ€‘k accuracy. |
| 4ï¸âƒ£  | `classifier_model.py` | ğŸ›ï¸ Commandâ€‘line orchestrator that wires everything together (GPU check, args, train, eval). |

---
## ğŸª„ Stepâ€‘byâ€‘Step Magic

### 1ï¸âƒ£ DataÂ Preparation Â ğŸ§¹
* **Load** raw JSONL (you tell us which columns are `text` âœï¸ & `label` ğŸ·ï¸).
* **Clean** with TFâ€‘IDF similarity â€“ goodbye duplicates & noise ğŸ‘‹.
* **Balance** the classes âš–ï¸ (optional label filters & downâ€‘sampling).
* **Split** into 80Â %Â train /Â 20Â %Â test âœ‚ï¸, keeping label proportions.
* **Upload** the two CSVs to a brandâ€‘new ğŸ¤© HuggingÂ Face dataset repo.

> ğŸ“¦ Outcome: `username/datasetâ€‘name` with perfectly prepped `train`/`test` splits.

### 2ï¸âƒ£ ModelÂ Fineâ€‘Tuning Â ğŸ‹ï¸â€â™‚ï¸
* **Download** your freshly minted dataset.
* **Map** labels to integers (`label2id`/`id2label`) â¡ï¸ ensures the model's head matches your classes.
* **Tokenize** every text up to 512Â tokens âœ‚ï¸ğŸ“.
* **Train** for 2 epochs using the HF `Trainer` (accuracy + macro/microÂ F1 tracked on WandB ğŸ“Š).
* **Push** the best checkpoint & tokenizer to `username/modelâ€‘name` on HF Hub ğŸš€.

### 3ï¸âƒ£ Evaluation Â ğŸ”
* **Instantiate** a `DebertaZeroShot` classifier with the pushed model.
* **Batchâ€‘predict** the heldâ€‘out test set (configurable batch size).âš™ï¸
* **Compute** ğŸ¯ Topâ€‘1, ğŸ¯ğŸ¯ Topâ€‘2, ğŸ¯ğŸ¯ğŸ¯ Topâ€‘3 accuracy (fixed bug only checks topâ€‘k list). 
* **Log** results to console *and* append perâ€‘example JSONL with predictions, scores & correctness flags ğŸ“„.

### 4ï¸âƒ£ Orchestration Â ğŸ›ï¸
* Single CLI command handles ğŸ’» GPU check, ğŸ”‘ tokens, file paths and flags.
* Defaults to *trainÂ âœÂ evaluate* workflow; add `--examples` to skip training & test an existing model instead.
* Produces everything you need: ğŸ“š dataset on Hub, ğŸ¤— model on Hub, ğŸ“ˆ WandB run, and a detailed results file.

---
## âœ¨ Why This Rocks

* **Reproducible** â€“ every artefact lives on the HuggingÂ Face Hub.
* **Balanced Data** â€“ no more skewed labels ruining your metrics.
* **Realâ€‘world Metrics** â€“ topâ€‘k accuracy shows if the right answer is "close" even when not rankâ€‘1.
* **Modular** â€“ swap the base model, tweak hyperâ€‘params, or plug in a new cleaner.
* **Handsâ€‘free** â€“ one command and the pipeline handles the rest. â˜•ï¸

Happy fineâ€‘tuning! ğŸ‰ 