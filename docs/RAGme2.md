# Knowledge Transfer: KNN-based RAG using `classifier_knn.py` üßëu200düè´ü§ù

## 1. Introduction & Purpose

This document provides a technical walkthrough of the Python script `code/classifier_knn.py`. This script serves multiple purposes within the ML pipeline, primarily centered around using **K-Nearest Neighbors (KNN) on text embeddings** for classification and analysis.

Crucially, the `predict_single_text` function within this script implements a form of **Retrieval-Augmented Generation (RAG)**. However, unlike some RAG systems that feed retrieved context into a large language model (LLM) for generation, this implementation uses the retrieved information (specifically, the labels of the nearest neighbors in the embedding space) to **directly classify** the input text.

The script handles:

*   **Embedding & Indexing:** Creating vector embeddings for text chunks and building a Faiss index for efficient searching.
*   **Data Filtering:** Creating balanced subsets of data (e.g., sparse representations) for training or analysis, often using Faiss itself during the filtering.
*   **Prediction (RAG):** Classifying new, unseen text based on the labels of its nearest neighbors in the embedding space.
*   **Testing & Evaluation:** Assessing the performance of the KNN classification approach.
*   **Visualization:** Generating UMAP plots for visualizing embedding distributions.

## 2. Core Component: `KnnEmbeddingClassifier`

While the main script orchestrates tasks, the core classification logic likely resides within the `KnnEmbeddingClassifier` class, imported from `libdocs.classifiers.knn.knn_classifier`. Although its source isn't fully visible here, we can infer its role based on usage:

*   **Initialization (`__init__(index_path)`):** When instantiated (e.g., `knn_classifier = KnnEmbeddingClassifier(index)`), it's responsible for loading the necessary components associated with the provided `index` path prefix:
    *   The pre-built **Faiss index** (`.faiss` file).
    *   The corresponding **embeddings** (`.npy` or similar file generated by `Embedder`).
    *   A **mapping** from index position / chunk ID back to the original `LabeledChunk` objects or their metadata (likely loaded from a `.jsonl` or pickle file saved alongside the index). This allows retrieving the *label* (e.g., `subject`) of a neighbor found via Faiss.
    *   An instance of the `Embedder` used to create the original embeddings, needed to embed new queries consistently.
*   **Prediction Method (e.g., `predict(query_embedding, k)` - *assumed name*):** This method (likely called internally by `predict_single_text`) would:
    1.  Take a query embedding and the number of neighbors `k`.
    2.  Use the loaded Faiss index to perform a KNN search.
    3.  Retrieve the original labels (e.g., `subject`) of the `k` nearest neighbors using the loaded ID/metadata map.
    4.  Apply a voting mechanism (e.g., majority vote) on the neighbor labels to determine the final predicted label.

## 3. Key Functions & RAG Workflow in `classifier_knn.py`

Let's break down the key functions within the script itself:

**a) Setup: `create_embeddings(model, index, labeled_chunks)`**

*   **Purpose:** This function performs the essential *indexing* step required before any KNN classification or RAG can occur. It builds the knowledge base.
*   **Workflow:**
    1.  Initializes an `Embedder` with the specified `model` (e.g., a Hugging Face model name).
    2.  Initializes a `FaissIndexer` with the correct embedding dimensions.
    3.  Takes a list of `labeled_chunks` (likely `LabeledChunk` objects containing text and a subject/label).
    4.  Uses the `Embedder` to generate vector embeddings for all input chunks.
    5.  Adds these embeddings and their corresponding `chunk.id`s to the `FaissIndexer`.
    6.  Saves the populated Faiss index to disk (`index + '.faiss'`).
    7.  Saves the raw embeddings (`index + '.npy'`) and potentially the chunk metadata map (`index + '.jsonl'`).
*   **Output:** The Faiss index and associated embedding/metadata files written to disk.

**b) RAG Query: `predict_single_text(index, text)`**

*   **Purpose:** This is the primary function implementing the KNN-based RAG for classifying a single piece of unseen text.
*   **Workflow:**
    1.  **Load:** Instantiates `KnnEmbeddingClassifier(index)`. This loads the Faiss index, embeddings, metadata map, and the appropriate embedder associated with the `index` path.
    2.  **Embed Query:** Uses the loaded classifier's internal `Embedder` to convert the input `text` into a query vector embedding. This ensures consistency with the indexed embeddings.
    3.  **Search (KNN):** Calls the classifier's internal prediction method (e.g., `knn_classifier.predict(query_embedding, k=5)` - *k value assumed*). This performs the Faiss search to find the `k` nearest neighbors to the query embedding.
    4.  **Retrieve & Vote:** The classifier retrieves the labels (e.g., `subject`) associated with the found neighbors using its internal metadata map.
    5.  **Classify:** It determines the most frequent label among the neighbors (majority vote) as the final prediction.
*   **Output:** Returns the predicted label (e.g., subject string) and potentially the neighbor details (IDs, distances, labels).

**c) Testing & Evaluation**

*   **`test_examples(...)`:** Loads a test dataset (likely `.jsonl`), iterates through examples, calls `predict_single_text` (or similar logic) for each, and compares the prediction to the true label.
*   **`test_and_print_accuracies(...)`:** Takes testing results (predictions vs. true labels) and calculates/prints accuracy metrics (overall accuracy, potentially per-class accuracy or confusion matrix).

**d) Data Filtering**

*   **`filter(...)`, `sparse_filter(...)`, `dense_filter(...)`:** These functions are used for **preprocessing** the data *before* creating the final index used for prediction. They aim to create more balanced or representative subsets of the data.
    *   `sparse_filter` is particularly interesting: It iterates through chunks of a specific subject and uses a temporary Faiss index *per subject* to ensure that newly added chunks are sufficiently *distant* (controlled by parameter `d`) from already included chunks in the embedding space. This prevents over-representation of dense clusters and creates a sparser, more diverse set of examples per subject, up to a `max_per_subject` limit.
    *   These are not part of the *query-time* RAG but are important for preparing the data that the RAG system searches over.

## 4. Configuration & Usage

Key parameters when running functions from this script:

*   `--index` or `index` argument: Specifies the base path/prefix for loading/saving the Faiss index, embeddings, and metadata map.
*   `--model`: Specifies the identifier for the embedding model to be used (passed to `Embedder`).
*   Input/Output files/dirs: Standard parameters for specifying data sources and destinations.
*   `k` (implicitly used in KNN): The number of neighbors to consider during prediction.

## 5. Summary

`code/classifier_knn.py` provides the tools to build and utilize a KNN-based classification system operating on text embeddings. The `predict_single_text` function acts as a RAG mechanism where:

1.  **Retrieval:** Is done via KNN search in a Faiss index of text chunk embeddings.
2.  **Generation/Output:** Is derived directly from the labels of the retrieved neighbors (classification) rather than generative prompting of an LLM.

This approach is effective for tasks where classifying documents based on semantic similarity to labeled examples is sufficient. The script also includes vital setup (indexing) and evaluation components. 